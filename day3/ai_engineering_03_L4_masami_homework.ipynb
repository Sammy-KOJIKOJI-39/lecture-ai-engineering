{"cells":[{"cell_type":"markdown","metadata":{"id":"D_6-jdBOXowG"},"source":["# 演習の方針\n","\n","1. **ベースラインモデル評価**  \n","   素のモデルで回答を生成し、講義内容との整合性の低さを観察します。これにより、特別な学習なしでのモデルの限界を確認します。\n","\n","2. **文字起こしデータの活用**  \n","   講義の文字起こしデータを導入し、モデルが講義内容を参照した回答を生成する傾向を観察します。ただし、Retrieval（情報検索）精度の限界から結果は不安定になる可能性があります。\n","\n","3. **チャンク化の導入**  \n","   文字起こしデータをチャンク（小単位）に分割し、より安定して関連コンテンツを取得できるようにします。この段階では文脈理解にまだ課題があることを確認します。\n","\n","4. **Rerankの適用**  \n","   検索結果のランク付けを導入し、より的確で安定した回答を目指します。\n","\n","5. **応用改善手法**  \n","   文字起こしの品質向上のための編集技術や、メタデータの活用による性能向上手法を探ります。"]},{"cell_type":"markdown","metadata":{"id":"PPI1pj4mFavt"},"source":["## 扱う質問\n","\n","「Inference Time Scaling（推論時スケーリング）」に関する質問を取り扱います。これは以下の背景を持つトピックです。\n","\n","- 2024年8月発表の論文「Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters」で提唱された概念\n","- OpenAIのGPT-o1（2024年9月リリース）で実用化され、注目を集めた比較的新しいアプローチ\n","- 2024年度LLM講座の第4回講義でも取り上げられた重要テーマ\n","\n","- 模範解答:\n","```\n","「Inference Time Scaling」とは、推論時に計算量を増やしてモデルの性能を高める手法です。これはモデルのサイズを大きくする代わりに、難しい入力に対して多くの計算リソースを使うことで、より良い出力を得ようとするアプローチです。\n","```\n","\n","## 扱うモデル\n","\n","Meta-Llama-3-8B-Instruct（2024年4月リリース）を使用します。このモデルは、リリース時期の関係上、以下の特徴を持ちます。\n","\n","- 「Inference Time Scaling」の概念が広まる前に訓練されており、このトピックに関する知識を持たないと想定される\n","- この特性を活かし、純粋なベースライン評価から各手法の効果を観察する"]},{"cell_type":"markdown","source":["\"八代目尾上菊五郎の襲名披露はどのような内容でしたか？\"\n","\n"],"metadata":{"id":"Pf5fMv7_mHI9"}},{"cell_type":"markdown","metadata":{"id":"eCGlMRPXSuim"},"source":["### 演習環境の準備"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"vM50WAI7GXwC","collapsed":true,"outputId":"b745df28-6da0-4551-baba-23a08193cb9f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746551768728,"user_tz":-540,"elapsed":6926,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n","Requirement already satisfied: google-colab-selenium in /usr/local/lib/python3.11/dist-packages (1.0.14)\n","Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (from google-colab-selenium) (4.32.0)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google-colab-selenium) (2.4.0)\n","Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (0.30.0)\n","Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (0.12.2)\n","Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (2025.4.26)\n","Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (4.13.2)\n","Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (1.8.0)\n","Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (25.3.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (2.4.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (3.10)\n","Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (1.3.0.post0)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (1.3.1)\n","Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium->google-colab-selenium) (1.2.0)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google-colab-selenium) (1.7.1)\n","Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->google-colab-selenium) (0.16.0)\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n","Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"]}],"source":["!pip install --upgrade transformers\n","!pip install google-colab-selenium\n","!pip install bitsandbytes"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_vs-Ri1Tslqw","outputId":"5ac4efdb-9977-4e86-84cc-124c78c7a5f1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746551774632,"user_tz":-540,"elapsed":105,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'lecture-ai-engineering' already exists and is not an empty directory.\n"]}],"source":["# 演習用のコンテンツを取得\n","!git clone https://github.com/matsuolab/lecture-ai-engineering.git"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"zXo_kFASXlvp","outputId":"42f6b2ab-443b-49c2-8fa5-e98e7422e353","colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["034d5e3b12fe47299d3551b2c6cc7ba9","0b89b22da8014d88b42e053f1948d733","74db5cad9fe34527baf7f471eeb44615","58acda839fac4aa0b4752394716c5a08","54fabe8a9d814cd9921d8f8ec379c043","6d1c373435b54749a7fad388845318da","2be7695d9d1e45a2b43537200b7ccbee","dc8a630efde045f7bcd93f45afd3f8ac","aa282e5cb1094cd39a71e13eaabd23d6","fa060b3dbffd4cbcb9cb1ed1fc9f7070","613982fde7664d47abcf79ff0f1763de","e805cf158a7341f0acda8a2f5d4a74cc","28aea689611842b6b3319c340567d0e9","0b898699a1ee4b3c9c206785a72df150","967c322ce9b442b5a04db26aad5b28b2","5d3643f6da5a48da883a9381df94d02d","cc0c003783d742ff8a503193a20af719","e68b5f2589424d268a38aea24e963320","bd1aea1142f54efa9f17c60311585c03","93ea6b73f32c4e8ba62ae6e63a0806ce"]},"executionInfo":{"status":"ok","timestamp":1746551775745,"user_tz":-540,"elapsed":230,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"034d5e3b12fe47299d3551b2c6cc7ba9"}},"metadata":{}}],"source":["# HuggingFace Login\n","from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"dZ_NUIftXwLc","executionInfo":{"status":"ok","timestamp":1746551796607,"user_tz":-540,"elapsed":2548,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[],"source":["# CUDAが利用可能ならGPUを、それ以外ならCPUをデバイスとして設定\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"7eTgV8XBPA90","executionInfo":{"status":"ok","timestamp":1746551796610,"user_tz":-540,"elapsed":2,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[],"source":["import random\n","random.seed(0)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"6tV9mO8oXoaM","outputId":"e49e4c4f-91ec-4e0a-fcd3-e497b3ba0543","colab":{"base_uri":"https://localhost:8080/","height":179,"referenced_widgets":["90392e6635944c9e85c0c0b47a7fc9f5","97733a01c952496d9e9301f3ec1a8462","779bdedbfcf8478b96401b142b95e066","54aa3ad26c024a1699f05e06134e1d7a","910e6f4076684b97bd353520f6f09b33","e0c43d43dadb4d83952dada98668bbe5","0409b85c4c6a4be9b1ac090ca13906e0","9e3302aa33fc4716ab2139264a897029","295ad0f4acff4ac3acc249bbf0b34fe4","19262dc7c9c64f0d801a3e1d0829d007","33318c18abc146fdb5daa5fd3a47849d"]},"executionInfo":{"status":"ok","timestamp":1746551822631,"user_tz":-540,"elapsed":25185,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90392e6635944c9e85c0c0b47a7fc9f5"}},"metadata":{}}],"source":["# モデル(Llama3)の読み込み\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","\n","model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=False,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","            model_name,\n","            device_map=\"auto\",\n","            quantization_config=bnb_config,\n","            torch_dtype=torch.bfloat16,\n","        )"]},{"cell_type":"markdown","metadata":{"id":"piTdVxTfGcc_"},"source":["# 1. ベースラインモデル評価\n","**まずはベースモデルがどの程度知識を持っているか確かめる**"]},{"cell_type":"code","source":["def generate_output(query, system_prompt=None):\n","  if system_prompt is None:\n","    messages = [\n","        {\"role\": \"user\", \"content\": query},\n","    ]\n","  else:\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": query},\n","    ]\n","  input_ids = tokenizer.apply_chat_template(\n","      messages,\n","      add_generation_prompt=True,\n","      return_tensors=\"pt\"\n","  ).to(model.device)\n","\n","  terminators = [\n","      tokenizer.eos_token_id,\n","      tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n","  ]\n","\n","  outputs = model.generate(\n","      input_ids,\n","      max_new_tokens=256,\n","      eos_token_id=terminators,\n","      do_sample=False,\n","      # temperature=0.6, # If do_sample=True\n","      # top_p=0.9,  # If do_sample=True\n","  )\n","\n","  response = outputs[0][input_ids.shape[-1]:]\n","  return tokenizer.decode(response, skip_special_tokens=True)"],"metadata":{"id":"OiBbJ9l54rAL","executionInfo":{"status":"ok","timestamp":1746551864962,"user_tz":-540,"elapsed":42,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["system_prompt = \"質問に回答してください。必ず「日本語で回答」すること。\"\n","question1 =  \"八代目尾上菊五郎の襲名披露はどのような内容でしたか？\"\n","question2 =  \"現在の尾上菊五郎は何代目ですか？\"\n","question3 =  \"菊之助が昼の部「京鹿子娘道成寺」で共演した役者の屋号は何ですか？\"\n","question4 =  \"なぜ七代目菊五郎は名前を変えないの？\"\n","question5 =  \"『稲瀬川勢揃い』で白拍子花子を演じたのは誰ですか？\"\n","\n","question = question5\n","response = generate_output(question, system_prompt)"],"metadata":{"id":"bhRVW3lo5COt","outputId":"497eb6d3-95fa-42fb-ca5f-83d21d501757","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746552934628,"user_tz":-540,"elapsed":1441,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"execution_count":50,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]}]},{"cell_type":"code","source":["print(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qEWDxcvdr3TC","executionInfo":{"status":"ok","timestamp":1746552934842,"user_tz":-540,"elapsed":9,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}},"outputId":"15ec4b18-7cb4-47d8-e747-25379fa7da4e"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["『稲瀬川勢揃い』で白拍子花子を演じたのは誰ですか？\n"]}]},{"cell_type":"code","source":["print(response)"],"metadata":{"id":"5PwMW9J-56Ad","outputId":"bd4c29f2-0a88-462a-c9f3-df6b8b7b0703","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746552937203,"user_tz":-540,"elapsed":13,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["稲瀬川勢揃いで白拍子花子を演じたのは、松本幸四郎です。\n"]}]},{"cell_type":"markdown","source":["- 数値的な評価も見てみます。RagasにはAnswer Accuracyという評価指標があります。今回はこちらを参考に実装した評価関数を利用して測っていきます。\n","\n","今回はLlama3では性能が不安定だったので、OpenAIのgpt-4oで評価していきます。従って、scoreの実行はopenAI APIキーを所持している関心がある方のみで良いです。"],"metadata":{"id":"fG9zI6_lAYsQ"}},{"cell_type":"code","source":["!pip install -U openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fncw05_I_IVl","outputId":"246d5888-4954-4e78-9d8d-494ce187e76c","collapsed":true,"executionInfo":{"status":"ok","timestamp":1746551890332,"user_tz":-540,"elapsed":2420,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.77.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"]}]},{"cell_type":"code","source":["# @title 評価実装\n","#gold_answer1 = \"「Inference Time Scaling」とは、推論時に計算量を増やしてモデルの性能を高める手法です。これはモデルのサイズを大きくする代わりに、難しい入力に対して多くの計算リソースを使うことで、より良い出力を得ようとするアプローチです。\"\n","gold_answer1 = (\n","    \"八代目尾上菊五郎の襲名披露は、2025年5月と6月に歌舞伎座で行われ、\"\n","    \"尾上丑之助が八代目尾上菊五郎を、尾上菊之助が六代目尾上菊之助を襲名しました。\"\n","    \"披露興行では『弁天娘女男白浪』の「浜松屋」「極楽寺屋根」「稲瀬川勢揃い」が上演され、\"\n","    \"弁天小僧菊之助の役を場面ごとに新菊五郎と新菊之助が演じ分ける演出が行われました。\"\n",")\n","gold_answer2 = (\n","    \"現在の尾上菊五郎は七代目と八代目です。\"\n",")\n","gold_answer3 = (\n","    \"音羽屋（おとわや）（八代目菊五郎）と大和屋（やまとや）（坂東玉三郎）です。\"\n",")\n","gold_answer4 = (\n","    \"通常は、八代目が襲名する際に、七代目は別の名跡を継ぐことで重複を避けますが、七代目は52年間「菊五郎」を名乗っており、自ら他の名跡を襲名する意向がないとされています。そのため、七代目と八代目が同時に「尾上菊五郎」を名乗る形となっています。\"\n",")\n","gold_answer5 = (\n","    \"『稲瀬川勢揃い』で白拍子花子を演じたのは六代目菊之助です。\"\n",")\n","\n","\n","#from openai import OpenAI\n","#from google.colab import userdata\n","#client = OpenAI(api_key=userdata.get(), max_retries=5, timeout=60)\n","import os\n","from openai import OpenAI\n","\n","# 入力からAPIキーを受け取って環境変数に保存（安全）\n","os.environ[\"OPENAI_API_KEY\"] = input(\"🔐 OpenAI APIキーを入力してください: \")\n","\n","client = OpenAI(\n","    api_key=os.environ[\"OPENAI_API_KEY\"],\n","    max_retries=5,\n","    timeout=60\n",")\n","\n","\n","def openai_generator(query):\n","\n","        messages = [\n","                    {\n","                        \"role\": \"user\",\n","                        \"content\": query\n","                    }\n","                ]\n","\n","        response = client.chat.completions.create(\n","            model=\"gpt-4o-mini\",\n","            messages=messages\n","        )\n","        return response.choices[0].message.content\n","\n","def evaluate_answer_accuracy(query, response, reference):\n","\n","    template_accuracy1 = (\n","          \"Instruction: You are a world class state of the art assistant for rating \"\n","          \"a User Answer given a Question. The Question is completely answered by the Reference Answer.\\n\"\n","          \"Say 4, if User Answer is full contained and equivalent to Reference Answer\"\n","          \"in all terms, topics, numbers, metrics, dates and units.\\n\"\n","          \"Say 2, if User Answer is partially contained and almost equivalent to Reference Answer\"\n","          \"in all terms, topics, numbers, metrics, dates and units.\\n\"\n","          \"Say 0, if User Answer is not contained in Reference Answer or not accurate in all terms, topics,\"\n","          \"numbers, metrics, dates and units or the User Answer do not answer the question.\\n\"\n","          \"Do not explain or justify your rating. Your rating must be only 4, 2 or 0 according to the instructions above.\\n\"\n","          \"Even small discrepancies in meaning, terminology, directionality, or implication must result in a lower score. Only rate 4 if the User Answer is a complete and precise match to the Reference Answer in every aspect.\\n\"\n","          \"### Question: {query}\\n\"\n","          \"### {answer0}: {sentence_inference}\\n\"\n","          \"### {answer1}: {sentence_true}\\n\"\n","          \"The rating is:\\n\"\n","      )\n","    template_accuracy2 = (\n","          \"I will rate the User Answer in comparison to the Reference Answer for a given Question.\\n\"\n","          \"A rating of 4 indicates that the User Answer is entirely consistent with the Reference Answer, covering all aspects, topics, numbers, metrics, dates, and units.\\n\"\n","          \"A rating of 2 signifies that the User Answer is mostly aligned with the Reference Answer, with minor discrepancies in some areas.\\n\"\n","          \"A rating of 0 means that the User Answer is either inaccurate, incomplete, or unrelated to the Reference Answer, or it fails to address the Question.\\n\"\n","          \"I will provide the rating without any explanation or justification, adhering to the following scale: 0 (no match), 2 (partial match), 4 (exact match).\\n\"\n","          \"Even minor inconsistencies in meaning, terminology, emphasis, or factual detail should prevent a rating of 4. Only assign a 4 if the User Answer exactly and unambiguously matches the Reference Answer in every respect.\"\n","          \"Do not explain or justify my rating. My rating must be only 4, 2 or 0 only.\\n\\n\"\n","          \"Question: {query}\\n\\n\"\n","          \"{answer0}: {sentence_inference}\\n\\n\"\n","          \"{answer1}: {sentence_true}\\n\\n\"\n","          \"Rating: \"\n","      )\n","\n","    score1 = openai_generator(\n","                template_accuracy1.format(\n","                      query=query,\n","                      answer0=\"User Answer\",\n","                      answer1=\"Reference Answer\",\n","                      sentence_inference=response,\n","                      sentence_true=reference,\n","                    )\n","                )\n","    try:\n","      score1 = int(score1)\n","    except:\n","      print(\"Failed\")\n","      score1 = 0\n","\n","    score2 = openai_generator(\n","                template_accuracy2.format(\n","                        query=query,\n","                        answer0=\"Reference Answer\",\n","                        answer1=\"User Answer\",\n","                        sentence_inference=reference,\n","                        sentence_true=response,\n","                    )\n","                  )\n","\n","    try:\n","      score2 = int(score2)\n","    except:\n","      print(\"Failed\")\n","      score2 = 0\n","\n","\n","    return (score1 + score2) / 2"],"metadata":{"id":"t89v938Y1o4I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746551906679,"user_tz":-540,"elapsed":12206,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}},"outputId":"a0b58739-0a75-405b-c16c-62c3174b07e2"},"execution_count":12,"outputs":[{"name":"stdout","output_type":"stream","text":["🔐 OpenAI APIキーを入力してください: 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\n"]}]},{"cell_type":"code","source":["# 評価\n","gold_answer = gold_answer5\n","score = evaluate_answer_accuracy(question, response, gold_answer)\n","print(score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CPLGyk7T5LaM","outputId":"cfaadcd1-2090-4764-8b7d-7697d5714050","executionInfo":{"status":"ok","timestamp":1746553015583,"user_tz":-540,"elapsed":1761,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0\n"]}]},{"cell_type":"markdown","source":["＊＊＊講義動画だと0.0だった"],"metadata":{"id":"nrE1rroItfyH"}},{"cell_type":"markdown","metadata":{"id":"ZSCNnRf9pJif"},"source":["## 結果 (ベースモデル)\n","\n","Meta-Llama-3-8B-Instructは「Inference Time Scaling」について誤った知識を提示しました：\n","* モデルは従来の「推論時間の短縮」という文脈でInference Time Scalingを解釈しており、これはLLM分野における最新の「Inference Time Scaling」概念（推論時計算資源の最適配分）とは異なる説明になります。\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"k4R-hiKNGyJd"},"source":["# 2. 文字起こしデータの活用\n","## 講義内容をソースとして活用 (RAG導入)\n","\n","モデルの回答の事実性を向上させるためにRetrieval Augmented Generation (RAG)技術を導入します：\n","\n","* **知識ソース**: LLM講座第4講における講師の発言内容\n","* **目的**: モデルに「Inference Time Scaling」に関する正確な知識と文脈を提供し、事実に基づいた回答を促す\n","\n","**初期RAG実装（ベーシックアプローチ）**:\n","* **ドキュメント処理**: 音声認識モデル(speech2text)で書き起こした生テキストをそのまま使用\n","* **分割方法**: 「。」（句点）で区切られた文単位でテキストを分割\n","* **検索手法**: シンプルな類似度ベースの検索でクエリに関連する文を抽出\n","* **制約条件**: モデルの入力トークン制限に収まるよう関連文のみを選択"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"47GvcceyObAl","executionInfo":{"status":"ok","timestamp":1746551928145,"user_tz":-540,"elapsed":10674,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","\n","#Pro課金しててもメモリ足りなくなったので軽量、精度良いらしいモデルに変更\n","#emb_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","emb_model = SentenceTransformer(\"infly/inf-retriever-v1-1.5b\", trust_remote_code=True)\n","\n","# In case you want to reduce the maximum length:\n","emb_model.max_seq_length = 8192"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"kPwggQfUS5yl","executionInfo":{"status":"ok","timestamp":1746551931625,"user_tz":-540,"elapsed":2,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[],"source":["#with open(\"/content/lecture-ai-engineering/day3/data/kabuki_kikugoro_202406.txt\", \"r\") as f:\n","#  raw_writedown = f.read()\n","#with open(\"/content/kabuki_kikugoro_202406.txt\", \"r\") as f:\n","#    raw_writedown = f.read()\n","\n","# txt_files = [\n","#     \"/content/kabuki_kikugoro_202406.txt\",\n","#     \"/content/yahoo_kikugoro_20250503.txt\",\n","#     \"/content/nhk_kikugoro_20250430.txt\",\n","#     \"/content/suponichi_kikugoro_20240527.txt\",\n","#     \"/content/dailyshincho_kikugoro_20240615.txt\",\n","#     \"/content/yagou.txt\",\n","#     \"/content/kabuki_danjyuro_20221031.txt\"\n","# ]\n","\n","# raw_writedown = \"\"\n","# for file_path in txt_files:\n","#     with open(file_path, \"r\") as f:\n","#         raw_writedown += f.read() + \"\\n\"\n","\n","txt_files = [\n","    \"data/kabuki_kikugoro_202406.txt\",\n","    \"data/yahoo_kikugoro_20250503.txt\",\n","    \"data/nhk_kikugoro_20250430.txt\",\n","    \"data/suponichi_kikugoro_20240527.txt\",\n","    \"data/dailyshincho_kikugoro_20240615.txt\",\n","    \"data/yagou.txt\",\n","    \"data/kabuki_danjyuro_20221031.txt\"\n","]\n","\n","raw_writedown = \"\"\n","for file_path in txt_files:\n","    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","        raw_writedown += f.read() + \"\\n\"\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"kxzKF6L2THIw","outputId":"5b6c42b8-bc7e-41a4-f822-5f98ba1d6a4c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746551933379,"user_tz":-540,"elapsed":4,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ドキュメントサイズ:  110\n","ドキュメントの例:\n"," それまで一所懸命、勤めてまいりたいと思います」と、切り出した菊五郎\n"]}],"source":["# ドキュメントを用意する。\n","documents = [text.strip() for text in raw_writedown.split(\"。\")]\n","print(\"ドキュメントサイズ: \", len(documents))\n","print(\"ドキュメントの例:\\n\", documents[min(5, len(documents) - 1)])\n","#print(\"ドキュメントの例: \\n\", documents[250])"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"nK4cYURzTHIx","outputId":"2f69bf2c-a958-4980-e42f-5ef188ab935c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746551949098,"user_tz":-540,"elapsed":12392,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[82.62371063232422, 75.76974487304688, 72.0406494140625, 66.09934997558594, 73.13088989257812, 66.4922103881836, 73.8149185180664, 67.04503631591797, 59.5385856628418, 66.6373062133789, 68.84681701660156, 74.73116302490234, 70.35856628417969, 63.71858596801758, 68.28219604492188, 74.16004180908203, 76.47210693359375, 73.64402770996094, 69.30390930175781, 66.75776672363281, 76.30276489257812, 65.1079330444336, 70.5672607421875, 65.42606353759766, 66.87956237792969, 70.30567169189453, 70.8396224975586, 67.68699645996094, 65.20950317382812, 59.574039459228516, 65.14596557617188, 60.27600860595703, 62.613773345947266, 80.95755767822266, 77.37615203857422, 65.44772338867188, 68.27005767822266, 62.06324768066406, 65.05724334716797, 69.16841125488281, 75.01132202148438, 69.47456359863281, 83.44642639160156, 80.44116973876953, 66.45339965820312, 77.69866943359375, 71.8258285522461, 67.89061737060547, 69.53357696533203, 75.2573471069336, 73.10227966308594, 74.81197357177734, 70.00373840332031, 75.85836029052734, 71.40904235839844, 71.6264877319336, 61.603267669677734, 64.99528503417969, 69.23287200927734, 64.20792388916016, 63.20851516723633, 64.1527328491211, 73.7635269165039, 61.40476608276367, 71.40428924560547, 72.11907196044922, 81.1045150756836, 64.65157318115234, 72.67261505126953, 69.97682189941406, 72.03752136230469, 69.1153793334961, 71.17742919921875, 64.63872528076172, 61.891639709472656, 62.330955505371094, 63.25359344482422, 62.002384185791016, 73.51007843017578, 77.30596160888672, 61.82923126220703, 70.04814910888672, 66.35476684570312, 61.099910736083984, 62.20970153808594, 64.25656127929688, 61.78917694091797, 56.66006851196289, 67.59284973144531, 74.54669189453125, 75.85374450683594, 63.35016632080078, 68.1412124633789, 72.07577514648438, 73.1507797241211, 69.10086822509766, 68.03105163574219, 67.73826599121094, 63.246707916259766, 61.184425354003906, 61.12197494506836, 72.11331176757812, 74.80181121826172, 69.50682067871094, 71.06400299072266, 64.58837127685547, 72.2866439819336, 64.57164001464844, 75.60819244384766, 61.616050720214844]]\n"]}],"source":["# Retrievalの実行\n","question1 = \"八代目尾上菊五郎の襲名披露はどのような内容でしたか？\"\n","question2 = \"現在の尾上菊五郎は何代目ですか？\"\n","question3 = \"菊之助が昼の部「京鹿子娘道成寺」で共演した役者の屋号は何ですか？\"\n","question4 = \"なぜ七代目菊五郎は名前を変えないの？\"\n","question5 = \"『稲瀬川勢揃い』で白拍子花子を演じたのは誰ですか？\"\n","\n","query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n","document_embeddings = emb_model.encode(documents)\n","\n","# 各ドキュメントの類似度スコア\n","scores = (query_embeddings @ document_embeddings.T) * 100\n","print(scores.tolist())"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"b_v8gx_tTHIx","outputId":"0536d792-03fa-40f3-987d-543264f9355f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746551951222,"user_tz":-540,"elapsed":6,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["取得したドキュメント1: (Score: 83.44642639160156)\n","【タイトル】\n","歌舞伎「八代目 尾上菊五郎」襲名 披露公演の成功を祈願\n","\n","【本文】\n","人気歌舞伎俳優の尾上菊之助さんが、江戸時代から続く大名跡の尾上菊五郎を八代目として正式に襲名し、襲名披露公演の成功を祈願する手打式に臨みました \n","\n","\n","取得したドキュメント2: (Score: 82.62371063232422)\n","【タイトル】\n","2025年5月、八代目尾上菊五郎、六代目尾上菊之助の襲名披露\n","\n","【本文】\n","2025年5月、6月、歌舞伎座で、尾上菊之助が八代目尾上菊五郎を、尾上丑之助が六代目尾上菊之助を襲名することが発表されました \n","\n","\n","取得したドキュメント3: (Score: 81.1045150756836)\n","来年5月に尾上菊之助（46）が「八代目尾上菊五郎」を襲名するが、実父の七代目尾上菊五郎（81）は、今後も同じ菊五郎を名乗り続けるというのである \n","\n","\n","取得したドキュメント4: (Score: 80.95755767822266)\n","▼\n","\n","　八代目尾上菊五郎、六代目尾上菊之助襲名披露興行は、来年の歌舞伎座「五月大歌舞伎」、「六月大歌舞伎」で行われ、さらに大阪松竹座「七月大歌舞伎」、10月御園座「吉例顔見世」、12月南座「吉例顔見世興行」、2026年の博多座「六月博多座大歌舞伎」へと続きます \n","\n","\n","取得したドキュメント5: (Score: 80.44116973876953)\n","29日、東京の歌舞伎座では、尾上菊之助さん（47）が八代目として菊五郎を、丑之助さんが六代目として父・菊之助の名跡を襲名し、来月からの襲名披露公演の成功を祈願する「古式顔寄せ手打式」が行われました \n","\n","\n"]}],"source":["topk = 5\n","for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n","  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n","  print(documents[index], \"\\n\\n\")"]},{"cell_type":"code","source":["references = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n","system_prompt = \"質問に回答してください。必ず「日本語で回答」すること。また、与えられる資料を参考にして回答すること。\"\n","#question =  f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"\n","question1 =  f\"[参考資料]\\n{references}\\n\\n[質問] 八代目尾上菊五郎の襲名披露の内容は？\"\n","question2 =  f\"[参考資料]\\n{references}\\n\\n[質問] 現在の尾上菊五郎は何代目ですか？\"\n","question3 =  f\"[参考資料]\\n{references}\\n\\n[質問] 菊之助が昼の部「京鹿子娘道成寺」で共演した役者の屋号は何ですか？\"\n","question4 =  f\"[参考資料]\\n{references}\\n\\n[質問] なぜ七代目菊五郎は名前を変えないの？\"\n","question5 =  f\"[参考資料]\\n{references}\\n\\n[質問] 『稲瀬川勢揃い』で白拍子花子を演じたのは誰ですか？\"\n","\n","question = question5\n","response = generate_output(question, system_prompt)"],"metadata":{"id":"jJujK9xx58qJ","outputId":"c7cf1e03-5461-4453-ffb5-dfd22503a69c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746553046613,"user_tz":-540,"elapsed":2674,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"execution_count":54,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]}]},{"cell_type":"code","source":["# print(question)\n","print(question.split(\"\\n\")[-1])  # 最後の質問行だけ表示"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V-BFGyx4tT2c","executionInfo":{"status":"ok","timestamp":1746553046622,"user_tz":-540,"elapsed":5,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}},"outputId":"1b98b92e-5638-41e5-ac44-f1ae95a5ceed"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["[質問] 『稲瀬川勢揃い』で白拍子花子を演じたのは誰ですか？\n"]}]},{"cell_type":"code","source":["print(response)"],"metadata":{"id":"xzeZErLk6cAH","outputId":"77b0cb93-9e33-4184-b72d-6fdf9d86f05d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746553047066,"user_tz":-540,"elapsed":5,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["参考資料に基づいて回答します。\n","\n","『稲瀬川勢揃い』で白拍子花子を演じたのは、八代目尾上菊五郎（尾上菊之助）です。\n"]}]},{"cell_type":"code","source":["# 評価 (openai apikeyがある場合のみ実行)\n","score = evaluate_answer_accuracy(question, response, gold_answer)\n","print(score)"],"metadata":{"outputId":"6ca60999-2dfc-47ff-ee2a-5030f67c066b","colab":{"base_uri":"https://localhost:8080/"},"id":"eEc_mCqp6npE","executionInfo":{"status":"ok","timestamp":1746553051206,"user_tz":-540,"elapsed":1550,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0\n"]}]},{"cell_type":"markdown","source":["＊＊＊講義動画だと1.0だった"],"metadata":{"id":"8VLC_OrA2QP2"}},{"cell_type":"code","source":["#for i, doc in enumerate(documents[:3]):\n","#    print(f\"[{i}]: {doc}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"prn5329in6sM","executionInfo":{"status":"ok","timestamp":1746535429914,"user_tz":-540,"elapsed":9,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}},"outputId":"a4d8d766-7827-4cf8-eda5-5102d3588e1a"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[0]: 2025年5月、八代目尾上菊五郎、六代目尾上菊之助の襲名披露\n","2025年5月、6月、歌舞伎座で、尾上菊之助が八代目尾上菊五郎を、尾上丑之助が六代目尾上菊之助を襲名することが発表されました\n","\n","[1]: ▼\n","\n","　来年5月、6月の歌舞伎座で、菊之助が八代目菊五郎を、丑之助が六代目菊之助を襲名、当代の菊五郎は変わらず七代目菊五郎として今後も舞台を勤めます\n","\n","[2]: 同時に襲名披露狂言も発表され、5月昼の部『京鹿子娘二人道成寺』で二人の白拍子花子を新菊五郎と新菊之助が、夜の部『弁天娘女男白浪』の弁天小僧菊之助を、「浜松屋」「極楽寺屋根」では新菊五郎、「稲瀬川勢揃い」では新菊之助が勤めます\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"Bn7tih0RTTzr"},"source":["## 結果 (初期RAG実装)\n","\n","講義内容のドキュメントを追加したにもかかわらず、モデルの回答には依然として以下の問題が見られます：\n","* 「高速に推論する」など、従来の一般的な推論最適化と「Inference Time Scaling」を混同した誤った解釈が継続\n","* 講義内容を参照しているものの、概念の本質を正確に捉えられていない\n","\n","### 問題分析\n","以下の要因が考えられます：\n","1. **ドキュメント品質の問題**: 音声認識による文字起こしの精度不足\n","2. **検索精度の課題**: 単純な文単位の分割では文脈が失われ、関連性の高いドキュメント片を適切に取得できていない可能性\n","\n","### 書き起こしテキストの品質改善\n","\n","日本語の音声認識（speech2text）モデルは精度に課題があることが知られています。以下に「LLMにおけるInference Time Scalingとは？」に関連する講義内容の書き起こしテキストを比較します："]},{"cell_type":"markdown","metadata":{"id":"q83QyfAIphk6"},"source":["### 講義中の該当発言 (LLM講座Day4後半から抜粋)\n","\n","\n","<修正前>\n","---\n","\n","講義に戻ります。ちょっと練習の時間もあるのであと20分ぐらいで駆け足になりますけど、最後最近のスケールトレンドって話で**生のGENIACLM**の話をして終わろうと思いですねちょっとモチベーションから話すと、ちょっと頭で考えてみてほしいとか見れば一瞬で思うとんですけどバナナの色は何ですかって言われたときと、今日の講義聞いた上で、**ゲームソフトの問題は何だと思いますか**って聞かれたとき、多分あの考えることが違うと思うんですね。**羽の色なんですか**っていうと一瞬黄色ですねもしかしたら緑かもしれないけどぐらいですかね物によるかなみたいなおもちゃだったら違うかもみたいな、だんだんあの、考えていくといろいろ出てくるかもしれないすけど、少なくとも**スケール足の問題なんだと思いますか**って聞かれたときに、今日の話からするとスケール則っていうのはこういうものだからどうだろうこの辺が問題かなみたいな考えとやっぱ思考としては違うってことは何となく思うかなと思います。なんか人間的にはこの二つって全然違うしあの、答えるのに必要な考え方っていうのも違うように思えるわけです。**スケールって言ってる7Gのスケール**って言ってるのはこういった形で、あの簡単なものについては簡単に答えてもいいですし、そうじゃなくて、あの考えなきゃいけない問題に対しては、考える時間を、に計算式を使うというふうにしたときに、これいいことがあるのかっていうような話になってます。二つで、ちょっと順番が前後しますけどこれの仕組みは言語モデルでも効果的ですかっていう話と、これをどう実現できるかっていう、こういう二つの話が最近のトレンドとして出てきています。効果的ですかっていうのが、最近**大湾**と呼ばれる論文が論文じゃないか、モデルが**オペル**から出ましたプレビューとして出てますけどこの法案で注目されていますこれあの**論文にROMってかブログ**にあるとイエスって右側が訓練時の計算資源をスケールさせたときに、初めて何かロジックのベンチマークがあるんですけどこれをがどうなったかで何となくスケールしてると右側がテストTimeコンピュートっていうふうに書いてると思うんすけど、**水温時**に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだんコース計算式を増やしていったときに、性能がどう変わるかっていうのでこれもスケールしていってるということがわかると思います。こういった形で、要は考える時間をどうやら推論時に使うと計算資源を推論使うのはいいことがありそうだということがわかります。\n","\n","\n","<修正後>\n","---\n","\n","\n","講義に戻ります。ちょっと演習の時間もあるのであと20分ぐらいで駆け足になりますけど、最後最近のスケールトレンドってことで**「推論時のスケーリング」**についての話をして終わろうと思います。モチベーションから話すと、ちょっと頭で考えてみてもらえれば一瞬でわかると思うとんですけど、「バナナの色は何ですかって言われたとき」と、今日の講義聞いた上で、**「スケール則の問題は何だと思いますか」**って聞かれたとき、多分あの考えることが違うと思うんですね。\n","**「バナナの色なんですか」**っていうと黄色ですね。もしかしたら緑かもしれないけど、物によるかなみたいな、おもちゃだったら違うかもみたいな、だんだんあの、考えていくといろいろ出てくるかもしれないすけど、少なくとも**「スケール則の問題なんだと思いますか」**って聞かれたときに、今日の話からするとスケール則っていうのはこういうものだから「どうだろう」「この辺が問題かな」みたいな考えとはやっぱ思考としては違うってことは何となく思うかなと思います。\n","なんか人間的にはこの二つって全然違うしあの、答えるのに必要な考え方っていうのも違うように思えるわけです。**推論時のスケールって言ってるのは**こういった形で、あの簡単なものについては簡単に答えてもいいですし、そうじゃなくて、深く考えなきゃいけない問題に対しては、考える時間に計算資源を使うというふうにしたときに、これいいことがあるのかっていうような話になってます。\n","これの仕組みは言語モデルでも効果的ですかっていう話と、これをどう実現できるかっていう、こういう二つの話が最近のトレンドとして出てきています。効果的ですかっていうのが、最近**o1**と呼ばれるモデルが**OpenAI**から出ました。プレビューとして出てますけどこのo1で注目されています。これあのo1の**論文ってかブログ**にある図で、左側が訓練時の計算資源をスケールさせたときに、AIMEというロジックのベンチマークがあるんですけど、accuracyがどうなったかというと、何となくスケールしてる。右側がtest-time computeっていうふうに書いてると思うんすけど、**推論時**に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだん計算資源を増やしていったときに、性能がどう変わるかっていうので、これもスケールしていってるということがわかると思います。\n","こういった形で、要は考える時間をどうやら推論時に使うと、つまり計算資源を推論時に使うのはいいことがありそうだということがわかります。\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qCrp81WzyhYc"},"source":["---\n","### 文字起こしの誤り\n","\n","上記の比較からわかるように、音声認識による書き起こしには重大な誤りが多数含まれています：\n","* 「スケール則の問題」→「ゲームソフトの問題」\n","* 「o1」→「大湾」\n","といった明らかに文脈に合わない単語変換が発生しています。\n","\n","`LLM2024_day4_raw.txt`の中には、このような誤変換が多数見られます。これらの誤りはRAG性能に直接影響し、モデルの回答精度を低下させる要因となります。\n","\n","したがって、**ドキュメント品質の改善**を行い、RAG性能の向上を図ります。\n","\n","## 講義内容をソースとして活用：改善版RAG実装\n","\n","* **ドキュメント処理**:\n","  - speech2textによる書き起こしテキストを人手で丁寧に修正\n","  - 専門用語（Inference Time Scaling、GPT-o1など）の正確な表記を確保\n","  - 文脈の流れを維持しつつ、文法的に正確な日本語に修正\n","\n","* **検索手法**:\n","  - 引き続き「。」（句点）で区切られた文単位でテキストを分割\n","  - 文単位の検索により、モデルの入力トークン制限内で関連情報を最大化\n","\n","この改善により、モデルが正確な情報に基づいて「Inference Time Scaling」の概念を理解し、適切な回答を生成することが期待されます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WNjIC4RnzkNW"},"outputs":[],"source":["with open(\"/content/lecture-ai-engineering/day3/data/LLM2024_day4.txt\", \"r\") as f:\n","  raw_writedown = f.read()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f53OojeTzkNW","outputId":"37c82e18-6a05-4ff8-fbeb-0a0143f34419","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746519699813,"user_tz":-540,"elapsed":4,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ドキュメントサイズ:  350\n","ドキュメントの例: \n"," それからBest of Nとはちょっと違う方法として、N個を生成した後に、それらを集約するという意味では、Day2でやったSelf-Consistencyをこの枠組みの一つとして説明されます\n"]}],"source":["# ドキュメントを用意する。\n","documents = [text.strip() for text in raw_writedown.split(\"。\")]\n","print(\"ドキュメントサイズ: \", len(documents))\n","print(\"ドキュメントの例: \\n\", documents[310])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlduigQ3OfoN","outputId":"e96d9ab2-779a-4f3f-c7b1-53775b74a8ff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746519709539,"user_tz":-540,"elapsed":7719,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[61.82579803466797, 66.46407318115234, 60.238338470458984, 51.8336067199707, 54.910701751708984, 51.560462951660156, 58.39971923828125, 60.80598068237305, 55.071990966796875, 59.28535461425781, 58.47712326049805, 62.86029815673828, 58.223690032958984, 59.106510162353516, 55.34396743774414, 57.35869598388672, 59.76055145263672, 51.49692153930664, 60.62431335449219, 62.524192810058594, 57.50200271606445, 58.56460189819336, 59.72063446044922, 59.440303802490234, 63.48786163330078, 61.78029251098633, 59.621185302734375, 62.459354400634766, 53.737796783447266, 61.90656280517578, 55.26152420043945, 56.99125671386719, 62.45269775390625, 56.23672103881836, 56.530357360839844, 56.4011344909668, 58.69480514526367, 51.75259017944336, 56.227516174316406, 56.70081329345703, 58.36994552612305, 55.44430160522461, 59.27916717529297, 59.5364990234375, 57.694984436035156, 54.76824188232422, 58.027793884277344, 57.40224838256836, 56.45027160644531, 52.43673324584961, 61.598026275634766, 62.70298767089844, 57.001686096191406, 59.716766357421875, 59.49424362182617, 56.54134750366211, 56.923973083496094, 57.18373489379883, 58.1857795715332, 58.07232666015625, 57.9510498046875, 57.586883544921875, 59.27503204345703, 55.39579391479492, 59.917686462402344, 54.54380416870117, 53.823341369628906, 57.40449523925781, 62.658119201660156, 56.264434814453125, 50.57676315307617, 60.73275375366211, 63.178165435791016, 54.546321868896484, 60.40806579589844, 62.065059661865234, 64.03901672363281, 58.506629943847656, 56.74992752075195, 60.13039779663086, 60.0478401184082, 65.01506805419922, 66.20873260498047, 60.68857955932617, 60.02225112915039, 56.822322845458984, 61.338035583496094, 59.941162109375, 61.12474060058594, 55.299983978271484, 62.15251159667969, 63.1419677734375, 59.728187561035156, 57.21210861206055, 56.25983428955078, 60.36614227294922, 57.48777389526367, 57.26992416381836, 58.51654815673828, 62.26036071777344, 58.37112045288086, 59.34796142578125, 53.603797912597656, 59.0064811706543, 52.59840393066406, 55.60133361816406, 61.407501220703125, 56.93905258178711, 54.407108306884766, 61.942405700683594, 55.815589904785156, 52.554954528808594, 61.49740219116211, 59.533119201660156, 59.60780334472656, 59.11349105834961, 57.18944549560547, 59.21208953857422, 61.62493896484375, 53.66834259033203, 54.285179138183594, 59.06537628173828, 61.03999710083008, 57.185997009277344, 58.598350524902344, 59.669654846191406, 61.457149505615234, 50.240821838378906, 52.52013397216797, 60.79863357543945, 59.19778823852539, 50.240821838378906, 51.40449905395508, 58.320098876953125, 58.717201232910156, 55.10460662841797, 55.5216064453125, 53.513275146484375, 58.32126235961914, 53.984893798828125, 57.045894622802734, 60.90895462036133, 60.3469123840332, 52.577728271484375, 55.29283142089844, 52.88685989379883, 61.41107177734375, 54.590309143066406, 60.88529968261719, 63.5172004699707, 60.72166061401367, 57.03959655761719, 58.82811737060547, 55.425418853759766, 55.18342971801758, 60.464561462402344, 63.97746276855469, 48.756553649902344, 52.58838653564453, 54.216732025146484, 55.07789993286133, 57.76833724975586, 58.6497917175293, 57.04724884033203, 52.212860107421875, 60.97160720825195, 52.46532440185547, 56.72309494018555, 59.38019561767578, 56.86973190307617, 55.26100540161133, 58.70604705810547, 58.05256271362305, 53.55756759643555, 52.730167388916016, 58.84788131713867, 58.912513732910156, 55.09020233154297, 56.79579544067383, 54.55907440185547, 58.08356857299805, 53.23056411743164, 56.833404541015625, 57.26052474975586, 57.21376419067383, 58.245147705078125, 56.68110656738281, 62.582923889160156, 57.18654251098633, 60.02552032470703, 60.727779388427734, 60.72771453857422, 60.269535064697266, 58.835350036621094, 56.8987922668457, 59.55915451049805, 64.6257553100586, 64.72672271728516, 61.05806350708008, 55.56776428222656, 62.3305549621582, 56.69194412231445, 57.033145904541016, 64.3873062133789, 54.65523147583008, 62.13454055786133, 59.661502838134766, 57.03208541870117, 57.64155578613281, 60.35540771484375, 59.75238037109375, 55.830848693847656, 60.0631103515625, 51.47021484375, 58.22126388549805, 58.971214294433594, 53.80593490600586, 57.6923713684082, 58.02265930175781, 56.06763458251953, 58.87961959838867, 55.528114318847656, 58.96348190307617, 57.67737579345703, 58.356239318847656, 53.343414306640625, 54.1365852355957, 57.74766159057617, 56.99241256713867, 58.2158203125, 56.841331481933594, 56.66986846923828, 56.96807098388672, 63.24211883544922, 52.39825439453125, 55.769168853759766, 59.11695861816406, 57.761436462402344, 52.15665054321289, 60.910369873046875, 58.45843505859375, 60.20790481567383, 59.19498062133789, 55.519615173339844, 63.66071701049805, 61.68472671508789, 65.29033660888672, 56.93702697753906, 60.184268951416016, 53.25228500366211, 62.614227294921875, 59.180484771728516, 63.69175720214844, 60.90964126586914, 57.66617202758789, 60.023582458496094, 57.251869201660156, 57.72660827636719, 55.75605773925781, 58.571044921875, 55.45200729370117, 57.769615173339844, 59.78694152832031, 59.30454635620117, 53.77749252319336, 59.057735443115234, 59.32798385620117, 65.19071197509766, 59.669315338134766, 61.35026168823242, 53.50626754760742, 64.77033233642578, 55.31249237060547, 48.97186279296875, 56.12712097167969, 53.85783386230469, 63.794654846191406, 58.53773498535156, 58.327423095703125, 53.340911865234375, 64.68262481689453, 66.58785247802734, 63.13125991821289, 62.02241134643555, 56.958831787109375, 59.115074157714844, 56.087459564208984, 65.24846649169922, 55.77976608276367, 58.781890869140625, 54.83138656616211, 54.26918411254883, 51.08735656738281, 61.758338928222656, 53.282649993896484, 59.69154739379883, 59.453975677490234, 56.83320617675781, 59.55241394042969, 54.366600036621094, 49.57448959350586, 58.468414306640625, 52.606510162353516, 56.50838088989258, 55.156253814697266, 63.68397903442383, 56.07685470581055, 58.67123031616211, 58.01264190673828, 55.93987274169922, 55.943565368652344, 56.81950378417969, 56.77760314941406, 56.58810806274414, 58.41075134277344, 53.491737365722656, 56.35892105102539, 58.1681022644043, 54.31625747680664, 57.036842346191406, 55.787174224853516, 58.8012580871582, 59.0906982421875, 59.22825241088867, 55.9727783203125, 58.38121032714844, 53.334999084472656, 53.97225570678711, 59.13868713378906, 54.19448471069336, 56.725738525390625, 54.17667770385742, 53.76275634765625, 56.58955001831055, 54.54039001464844, 57.886653900146484, 54.97636795043945, 57.51332092285156, 56.35451126098633, 55.35363006591797, 57.47932052612305, 57.39678192138672, 54.425811767578125, 54.131412506103516, 53.86015319824219, 64.7950668334961, 58.54460144042969, 67.38101196289062, 55.74285888671875, 50.88466262817383]]\n"]}],"source":["# Retrievalの実行\n","question = \"LLMにおけるInference Time Scalingとは？\"\n","\n","query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n","document_embeddings = emb_model.encode(documents)\n","\n","# 各ドキュメントの類似度スコア\n","scores = (query_embeddings @ document_embeddings.T) * 100\n","print(scores.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FNsGUsnlOoMm","outputId":"a41da5dc-c209-431e-b10c-bd5afa04ac4b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746519709548,"user_tz":-540,"elapsed":3,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["取得したドキュメント1: (Score: 67.38101196289062)\n","最後に補足して僕のパート終わろうと思いますけど、同じ計算資源のときにパラメータ増やすのよりも推論資源を増やすのが有効なのかっていうのが問いとしてあると思いますけど、o1の場合だと、訓練時のスケールは同じままって推論時のスケールを増やしたら、より賢くなりましたって話でしたけど、どっちにするのがいいのかっていう意味で言うと、GoogleDeepMindが8月に論文としてまして、Scaling LLM Test-Time Comupte Optimally can be more Effective than Scaling More Paremetersっていうことで、良いらしいというふうに言われてます \n","\n","\n","取得したドキュメント2: (Score: 66.58785247802734)\n","右側がtest-time computeっていうふうに書いてると思うんすけど、推論時に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだん計算資源を増やしていったときに、性能がどう変わるかっていうので、これもスケールしていってるということがわかると思います \n","\n","\n","取得したドキュメント3: (Score: 66.46407318115234)\n","あのスケールするっていうところではタイトルの通りなんですけど、ちょっとこれスケーリングPretraining回ってなってるんですけれども、ちょっと最近はですね、このPretrainingだけではなくて、推論をスケールさせるというような話も出てきてましてせっかくなのでその最近の話題ということです推論時のスケーリングことで、ちょっとタイトル詐欺が入ってるんですけどPretrainingだけじゃない、スケーリングも扱うということで、ちょっと若干あのタイトル詐欺なんですけども、あの最近の話題ということで推論時のスケジュールについても話していきたいなと思っています \n","\n","\n","取得したドキュメント4: (Score: 66.20873260498047)\n","Trasnformerの場合はスケール則が、パラメータ数が横軸になってますけどこういうふうになると、LSTMの場合には1層2層4層みたいにそれぞれスケール則を解くとこんなふうになりますよということで、Trasnformer以外のスケール則っていうのもあの検証をされている \n","\n","\n","取得したドキュメント5: (Score: 65.29033660888672)\n","気にしながらっていうのの実例を出した方がわかりやすいと思うので、実際にこれ開発者じゃないので、あの結果を見て推論してるだけなんで嘘ついてるかもしれないですけど例えばLlama3の論文を持ってくると8Billon,70Billon,405Billonで層の数、モデルDimension、埋め込みの数次元ですね、フィードフォワードの次元、アテンションの数っていうのを、こういうふうにしたよっていうふうに言われてます \n","\n","\n"]}],"source":["topk = 5\n","for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n","  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n","  print(documents[index], \"\\n\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MoOCvFW4ltcA","outputId":"4b171cde-814b-459a-bbf1-af521ce3f60e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746519721273,"user_tz":-540,"elapsed":11725,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]}],"source":["references = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n","system_prompt = \"質問に回答してください。必ず「日本語で回答」すること。また、与えられる資料を参考にして回答すること。\"\n","question =  f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"\n","response = generate_output(question, system_prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FbzMLfTtWxx","outputId":"8ff66a9a-f869-44f7-ba3f-b53f88fde967","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746519721285,"user_tz":-540,"elapsed":10,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["LLM（Large Language Model）におけるInference Time Scalingは、推論時（Inference）に計算資源をスケーリングすることで、モデルがより賢くなり、性能が向上することを意味します。Google DeepMindの論文「Scaling LLM Test-Time Compute Optimally can be more Effective than Scaling More Parameters」では、推論時計算資源をスケーリングすることが、パラメータ数を増やすよりも有効な方法であると示されています。\n","\n","Inference Time Scalingは、TrasnformerやLSTMなどのモデルにおいて、パラメータ数を増やすのではなく、推論時計算資源をスケーリングすることで、性能が向上することを目指します。例えば、Llama3の論文では、8億、70億、405億という異なるスケールで、モデルが性能を向上させることが示されています。\n","\n","Inference Time Scalingは、最近の話題であり、Pretrainingだけでなく、推論時スケーリングも重要なトピックとなっています。\n"]}],"source":["print(response)"]},{"cell_type":"code","source":["# 評価 (openai apikeyがある場合のみ実行)\n","score = evaluate_answer_accuracy(question, response, gold_answer)\n","print(score)"],"metadata":{"outputId":"4d0234aa-55be-42a0-91a4-57aedf76df0e","colab":{"base_uri":"https://localhost:8080/"},"id":"9nBfE52t7jJr","executionInfo":{"status":"ok","timestamp":1746519722581,"user_tz":-540,"elapsed":1272,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.0\n"]}]},{"cell_type":"markdown","source":["＊＊＊講義動画だと2.0だった"],"metadata":{"id":"GVEhek7x2pt3"}},{"cell_type":"markdown","metadata":{"id":"vLe0IJPeH97d"},"source":["## 結果 (修正テキストによるRAG)\n","\n","書き起こしテキストの品質改善により、モデルの回答に部分的な向上が見られました：\n","\n","### 改善点\n","* 「推論時（Inference）に計算資源をスケーリングすることで、モデルがより賢くなり、性能が向上すること」という概念を正確に捉えるようになった\n","\n","### 問題点\n","* 「Inference Time Scalingは、TransformerやLSTMなどのモデルにおいて、パラメータ数を増やすのではなく、推論時計算資源をスケーリングすることで、性能が向上すること...」という記述は講義内容と矛盾している\n","\n","### 問題分析\n","\n","モデルが誤った回答を生成する主要因として、**文脈の欠如**が考えられます：\n","* 「。」で区切られた短い文単位での検索では、各文の発言背景や関連性が失われる\n","* 単独の文から情報を抽出するため、講師の全体的な主張や議論の流れを把握できない\n","* 結果として、正しい個別の文でも、その解釈に必要な背景情報が欠如し、誤った文脈で理解される"]},{"cell_type":"markdown","source":["# 3. 文脈を考慮したチャンク化の導入\n","\n","検索結果の品質向上のため、以下の改善を実施します：\n","\n","* **前後文脈を含むチャンク化**:\n","  - 検索でマッチした文だけでなく、その前後の複数文も含めてチャンクとして取得\n","  - 具体的には、マッチした文を中心に前2文、後2文を含む計5文程度のチャンクを構成\n","  - この「文脈ウィンドウ」により、発言の背景情報や議論の流れが保持される\n","\n","* **期待される効果**:\n","  - 講師の主張とその根拠の関係性を正確に把握できる\n","  - 概念の定義とその適用範囲を正しく理解できる\n","\n","この改善により、モデルが講義内容の本質をより正確に理解し、一貫性のある事実に基づいた回答を生成することが期待されます。"],"metadata":{"id":"cydr_gASBU7h"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"94uovDFrVOTJ","outputId":"99375a2f-c076-4a48-929d-e3523230944b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746519874548,"user_tz":-540,"elapsed":9551,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]}],"source":["# 前後それぞれ2つずつの文章を一つのドキュメントに追加する。（要は5つの文章集合になる)\n","references = \"\\n\".join([\"* \" + \"。\".join(documents[max(0, i-2): min(i+2, len(documents))]).strip() for i in scores.argsort()[0][::-1][:topk]])\n","system_prompt = \"質問に回答してください。必ず「日本語で回答」すること。また、与えられる資料を参考にして回答すること。\"\n","question =  f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"\n","response = generate_output(question, system_prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"4078581a-433a-41be-bdca-b7e0561057ea","colab":{"base_uri":"https://localhost:8080/"},"id":"mL_UhkfEBd0-","executionInfo":{"status":"ok","timestamp":1746519874560,"user_tz":-540,"elapsed":11,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["LLM（Large Language Model）におけるInference Time Scalingは、推論時スケーリングのことです。すなわち、既に学習された言語モデルを、より大きなスケールで推論することで、性能を向上させることを目指しています。\n","\n","Inference Time Scalingは、LLMの推論時計算量を増やすことで、より高精度の推論結果を得ることを目的としています。具体的には、LLMのパラメータ数を増やすことで、より複雑な計算を実行できるようになり、より高精度の推論結果を得ることができます。\n","\n","この概念は、OpenAIの研究で提唱されており、LLMの推論時スケーリングを実現することで、性能を向上させることができることが示されています。\n"]}],"source":["print(response)"]},{"cell_type":"code","source":["# 評価 (openai apikeyがある場合のみ実行)\n","score = evaluate_answer_accuracy(question, response, gold_answer)\n","print(score)"],"metadata":{"outputId":"b06afc33-5f14-46a5-f8ae-6ee5c22b6e80","colab":{"base_uri":"https://localhost:8080/"},"id":"aONZ9fT4Bd0_","executionInfo":{"status":"ok","timestamp":1746519875999,"user_tz":-540,"elapsed":1438,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.0\n"]}]},{"cell_type":"markdown","source":["＊＊＊講義動画だと2.0だった"],"metadata":{"id":"HKbsnjal3DaM"}},{"cell_type":"markdown","metadata":{"id":"CD3R54G1WX8B"},"source":["## 結果 (文脈付きチャンク化によるRAG)\n","\n","文脈を含むチャンク化により、モデルの回答の方向性に明確な改善が見られました：\n","\n","### 改善点\n","* 「推論時の計算をスケールさせる」という概念を据えて回答\n","* Inference Time Scalingの基本原理についての理解が向上\n","\n","### 残存する問題点\n","* 質問と関連性の低い情報（ノイズ）が混入する\n","\n","### 問題分析\n","\n","文脈付きチャンク化によるアプローチで新たに発生した課題：\n","\n","1. **情報過多の問題**:\n","   * ドキュメント量の増加により、モデルに提供される情報総量が大幅に増加\n","   * 関連情報と非関連情報が混在し、ノイズと重要情報の区別が困難に\n","\n","2. **情報選択の複雑化**:\n","   * モデルは単に回答を生成するだけでなく、提供された多様な情報源から関連性の高い情報を選別する作業も担うことになった\n","   * この二重タスクにより回答生成の難易度が上昇"]},{"cell_type":"markdown","source":["# 4. Rerankによる情報品質の向上\n","\n","検索精度をさらに向上させるため、二段階の検索プロセスを導入します：\n","\n","* **Rerank手法の導入**:\n","  - 第一段階: 従来通り基本的な検索アルゴリズムでtop-k個のドキュメントチャンクを取得\n","  - 第二段階: 取得したチャンクに対してLLMを活用した高度な関連性評価を実施\n","  - LLMに「このドキュメントは質問『LLMにおけるInference Time Scalingとは？』に対して本当に関連性が高いか」を判断させる\n","  - 関連性スコアに基づいてランク付けし、真に関連性の高いチャンクのみを選出\n","\n","* **期待される効果**:\n","  - 質の高い情報に焦点を絞ることで、ノイズとなる情報を大幅に削減\n","  - 文脈を保ちながらも、関連性の高い情報のみをモデルに提供\n","  - モデルのタスクを「多量の情報から選別して回答」から「厳選された情報に基づいて回答」へと単純化\n","\n","この高度な情報フィルタリングにより、Inference Time Scalingに関する正確で一貫性のある回答生成が期待されます。"],"metadata":{"id":"sedbYERHBmx1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HfzJ5EpXGtj","outputId":"17617a0b-9a60-4953-9932-963b0b94990d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746519978437,"user_tz":-540,"elapsed":1427,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","対象となるドキュメント:\n"," これでほぼちょうどですけど、最後に少しあの、前半では全体の訓練時のスケーリングをする話を基本的にしましたけど、最近ではこの推論時の計算量っていうのも注目するような研究が増えてきています。\n","代表的なGPT-o1とかですごく注目されてるかなと思いますし、今までやった方法、学んだ方法も結構出てきたと思いますけど、Promptingを工夫するとか、Decodingを工夫するとかいうので、それにも発展的な方法がいろいろ出てきていますし、Meta Generationっていうような枠組みで、DecodingだけじゃなくてそのDecodeした結果を最後どう使うかみたいな含めて、Meta Generationというふうに呼んでますけど、Paralell SearchとかStep Level SearchとかRefinementと言われるような枠組みの研究も出てきていますというような話をしました。\n","最後に補足して僕のパート終わろうと思いますけど、同じ計算資源のときにパラメータ増やすのよりも推論資源を増やすのが有効なのかっていうのが問いとしてあると思いますけど、o1の場合だと、訓練時のスケールは同じままって推論時のスケールを増やしたら、より賢くなりましたって話でしたけど、どっちにするのがいいのかっていう意味で言うと、GoogleDeepMindが8月に論文としてまして、Scaling LLM Test-Time Comupte Optimally can be more Effective than Scaling More Paremetersっていうことで、良いらしいというふうに言われてます。\n","厳密に言うとこれなんかタスクによって違うということなので、良いとまで言っていいのかちょっと若干誇大広告な気が個人的にはしてますけど、そういったことを検証するような研究も出てきていますので興味ある人は見てもらえばと思います\n","\n","関連しているかどうか:  yes\n","\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","対象となるドキュメント:\n"," プレビューとして出てますけどこのo1で注目されています。\n","これあのo1の論文ってかブログにある図で、左側が訓練時の計算資源をスケールさせたときに、AIMEというロジックのベンチマークがあるんですけど、accuracyがどうなったかというと、何となくスケールしてる。\n","右側がtest-time computeっていうふうに書いてると思うんすけど、推論時に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだん計算資源を増やしていったときに、性能がどう変わるかっていうので、これもスケールしていってるということがわかると思います。\n","こういった形で、要は考える時間をどうやら推論時に使うと、つまり計算資源を推論時に使うのはいいことがありそうだということがわかります\n","\n","関連しているかどうか:  yes\n","\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","対象となるドキュメント:\n"," 早速内容ですけど、目的はタイトルの通りですけど言語モデルスケール則について学ぶってことで、大規模言語モデルっていうふうに呼ばれてますけど、ちょっとスケール則の話とか初回も少ししましたけど、これだけ大きくなっている一つの理由になってますのでそのスケール則ってどういうものなのかとかそれがなぜ重要なのかっていうところ、説明できるようなってもらうというところと、スケール則ってどうやって求めるんでしたっけというところを説明実装できるようになるところについて中心的に話していければと思ってます。\n","あのスケールするっていうところではタイトルの通りなんですけど、ちょっとこれスケーリングPretraining回ってなってるんですけれども、ちょっと最近はですね、このPretrainingだけではなくて、推論をスケールさせるというような話も出てきてましてせっかくなのでその最近の話題ということです推論時のスケーリングことで、ちょっとタイトル詐欺が入ってるんですけどPretrainingだけじゃない、スケーリングも扱うということで、ちょっと若干あのタイトル詐欺なんですけども、あの最近の話題ということで推論時のスケジュールについても話していきたいなと思っています。\n","演習では2つ目のポイントに近いですけどスケール則を実際に求めるというところでそのコードを実装できるようになってもらうということを目的としています\n","\n","関連しているかどうか:  yes\n","\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","対象となるドキュメント:\n"," それをスケールアップさせたのが先ほどのOpenAIの研究だというふうに説明できるかなと思います。\n","それから元のOpenAIの論文に戻りますと、今言ったようなLSTMの比較みたいなLSTMにおけるスケール則みたいなことも、この論文でも検証されていまして、左側がモデル構造が違うんですね。\n","Trasnformerの場合はスケール則が、パラメータ数が横軸になってますけどこういうふうになると、LSTMの場合には1層2層4層みたいにそれぞれスケール則を解くとこんなふうになりますよということで、Trasnformer以外のスケール則っていうのもあの検証をされている。\n","深さについても検証してまして、これも元のモデルが何だったかちょっと忘れちゃったけど、確かLSTMだったような気がしますけど、層を変えたときにどういうふうな変化するかっていうのをこういった形でプロットするようなことがされてます\n","\n","関連しているかどうか:  no\n","\n","\n","\n","\n","\n","対象となるドキュメント:\n"," こういったものを変えて実験してみたっていうのが最初の最初じゃないOpenAIのScaling Lawで話されていました。\n","基本的にはこの辺見るとなんかあんまり性能に影響ないっていうふうにこの論文では言ってますけど、この辺を気にしながらモデルスケールすることが多いです。\n","気にしながらっていうのの実例を出した方がわかりやすいと思うので、実際にこれ開発者じゃないので、あの結果を見て推論してるだけなんで嘘ついてるかもしれないですけど例えばLlama3の論文を持ってくると8Billon,70Billon,405Billonで層の数、モデルDimension、埋め込みの数次元ですね、フィードフォワードの次元、アテンションの数っていうのを、こういうふうにしたよっていうふうに言われてます。\n","これさっき言ったアスペクト比、縦横比がこのモデルdimentionをLayerで割ったものなんで、これそれぞれ見ると128,102.4,130ってことでこれ大体100から130ぐらい、なんかおおむね同じような値になっていることがわかると思います\n","\n","関連しているかどうか:  no\n","\n","\n","\n"]}],"source":[" #回答に役立つ該当の発言はreference[1871]〜に含まれてます。\n","references = []\n","for ref in [\"。\".join(documents[max(0, i-2): min(i+2, len(documents))]).strip() for i in scores.argsort()[0][::-1][:topk]]:\n","\n","  system_prompt = \"与えられた参考資料が質問に直接関連しているか？'yes''no'で答えること。ただし、余計なテキストを生成しないこと。\"\n","  question =  f\"[参考資料]\\n{ref}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"\n","  response = generate_output(question, system_prompt)\n","\n","  print(\"\\n\\n対象となるドキュメント:\\n\", ref.replace(\"。\", \"。\\n\"))\n","  print(\"\\n関連しているかどうか: \", response)\n","\n","  if \"yes\" in response.lower():\n","    references.append(ref)\n","\n","  print(\"\\n\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLietDaD5I3h","outputId":"82dd9b0a-1332-4a90-9cf4-968d5e681ad9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746519980943,"user_tz":-540,"elapsed":11,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["3\n"]}],"source":["print(len(references))"]},{"cell_type":"markdown","metadata":{"id":"Fs74h4ADXj99"},"source":["上記より、上位3件のみが関連しているとわかったので、これらだけをモデルに渡すこととする。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fu9wBykZXxja","outputId":"8237eec0-c82e-4015-d340-bb259a83d64d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746519992919,"user_tz":-540,"elapsed":10916,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]}],"source":["system_prompt = \"質問に回答してください。必ず「日本語で回答」すること。また、与えられる資料を参考にして回答すること。\"\n","question =  f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"\n","response = generate_output(question, system_prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5kHntvSXxjb","outputId":"f6a21d97-0c6f-4712-fee2-baa1e3cea583","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746519992923,"user_tz":-540,"elapsed":3,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["LLM（Large Language Model）におけるInference Time Scalingは、推論時（Inference）に計算資源をスケールさせることで、性能を向上させることを指します。つまり、LLMが推論する際に、計算資源を増やすことで、より正確な結果を得ることができます。\n","\n","この概念は、Google DeepMindの論文「Scaling LLM Test-Time Compute Optimally can be more Effective than Scaling More Parameters」に基づいています。この論文では、LLMの推論時計算資源をスケールさせることで、性能を向上させることができ、パラメーターの増やすよりも有効なことが示されました。\n","\n","Inference Time Scalingは、LLMの推論時計算資源をスケールさせることで、性能を向上させることを目的としています。具体的には、LLMが推論する際に、計算資源を増やすことで、より正確な結果を得ることができます。\n"]}],"source":["print(response)"]},{"cell_type":"code","source":["# 評価 (openai apikeyがある場合のみ実行)\n","score = evaluate_answer_accuracy(question, response, gold_answer)\n","print(score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Wo1v-V7CEBv","outputId":"98c38332-ad79-4171-9628-3b53e92e274e","executionInfo":{"status":"ok","timestamp":1746519994099,"user_tz":-540,"elapsed":1176,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.0\n"]}]},{"cell_type":"markdown","source":["＊＊＊講義動画だと3.0だった"],"metadata":{"id":"Mi-XrnLutPCV"}},{"cell_type":"markdown","metadata":{"id":"elqD2gJt5RCo"},"source":["## 結果 (Rerank導入後)\n","\n","Rerankの導入により、回答品質に改善が見られました：\n","\n","### 達成された成果\n","* Inference Time Scalingに関する正確な情報を含んだ回答の生成\n","* 無関係な情報やノイズの排除\n","* 講義内容を反映した説明の実現 🎉\n","\n","この結果から、RAGパイプラインにおける情報の質と関連性の重要性であり、検索で取得した情報を単に増やすだけでなく、その情報の関連性を精査する方法を学ぶことができました。\n","\n","---\n","\n","\n","# 5. さらなる改善案: 意味的チャンク化\n","\n","文単位での分割と前後文脈の追加という現在のアプローチをさらに発展させる手法として、**意味的なチャンク化**が考えられます：\n","\n","* **意味的チャンク（段落）単位での分割**:\n","  - 単純な文の区切りではなく、意味的なまとまり（トピック、議論、例示など）に基づいてテキストを分割\n","  - 人間の主観に基づく意味的な段落分けを活用\n","  - 各チャンクが「一つの完結した考え」を表現するようにする\n","\n","* **期待される効果**:\n","  - より自然な文脈理解が可能に（人間の思考や会話の流れに近い）\n","  - トピックの開始から結論までの流れを維持できる\n","  - 概念間の関係性や比較が同一チャンク内に含まれ、より深い理解につなげる\n","\n","* **検証方法**:\n","  - 人間が主観的に意味でグループ化したチャンクセットを用意\n","  - 同じRerank手法を適用し、文単位チャンクとの性能差を比較\n","  - 回答の正確性、一貫性、網羅性を評価指標として使用\n","\n","この意味的チャンク化手法は、特に講義のような構造化された発話においては、より自然で効果的な情報検索と理解を可能にすると予想されます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4DNOyPNXAtl3","outputId":"510c9e79-e0dd-4857-d442-97e6e4e861db","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746519999358,"user_tz":-540,"elapsed":18,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ドキュメントサイズ:  45\n","ドキュメントの例: \n"," 具体的な求め方についても話します。 さっきからチラチラ言ってた通りなんすけど基本的にこれどう図るかっていうと、基本的にはいくつかの条件で実験してフィッティングするって言ってんのは、すごい単純に言ってしまうとそうなります。左側GPT4の論文から取ってきた図で説明したもんですけど、グレーのやつを例えば実験してみて、これぐらいのロスになるんだなっていうので、フィッティングするとこういうカーブになります。 ちなみにこれ、なんでこれ直線にならないんだっていうのをすぐ説明しなかったですがこれ縦軸が実は普通のロスと違ってBits-per-wordっていうのになってて、多分2乗スケールのロスになってるからだと思います。 右側も同じですね。この各点について何かいろんな設定で実験してやって、それを結果を見るということをしてますけどよくよく考えるとスケールさせるときにモデルサイズどうすればいいんでしたっけとか、何をどういじるとモデルサイズが大きくなるんでしたっけ、どういうふうに言えばいいんでしたっけとかですね。 あのモデルサイズ変えたときにハイパーパラメータってどうすんでしたっけそういった細かい問題が出てくる。最初の方ですけどモデルサイズどう変化させるかっていうので、前回やった、こういう図があると思いますけどモデルサイズ変えようと思ったら別にパラメータ、層の数を増やしても、いいわけですし、この埋め込みの次元各tokenの次元を増やしてもいいわけですし、各随所に出てくるこのフィードフォワードネットワークっていうのの中間層の次元を上げてもいいですしヘッドを増やしてもそういうのあのパラメータ自体は上がるということで、これどれをどのぐらいやるんですかっていうのが細かく考えると重要になってきます。 この辺は元の論文でも一応議論されてまして、これ三つほど出してるんすけど例えば真ん中のがアスペクト比っていう、モデルのエンベディングのサイズですね。dモデルっていうものを層数で割ったもの、アスペクト比という縦横比みたいなもので幅と深さの比率をアスペクト比っていうふうにこの論文では呼んでいますけど。こういったものを変えて実験してみたっていうのが最初の最初じゃないOpenAIのScaling Lawで話されていました。基本的にはこの辺見るとなんかあんまり性能に影響ないっていうふうにこの論文では言ってますけど、この辺を気にしながらモデルスケールすることが多いです。 気にしながらっていうのの実例を出した方がわかりやすいと思うので、実際にこれ開発者じゃないので、あの結果を見て推論してるだけなんで嘘ついてるかもしれないですけど例えばLlama3の論文を持ってくると8Billon,70Billon,405Billonで層の数、モデルDimension、埋め込みの数次元ですね、フィードフォワードの次元、アテンションの数っていうのを、こういうふうにしたよっていうふうに言われてます。 これさっき言ったアスペクト比、縦横比がこのモデルdimentionをLayerで割ったものなんで、これそれぞれ見ると128,102.4,130ってことでこれ大体100から130ぐらい、なんかおおむね同じような値になっていることがわかると思います。 それからモデルとフィードフォワードの次元数ですね、モデル次元数に対しフィードフォワードの次元数は3.5倍になっているということがわかります。これ約3.5かな。ちょっと自信ないですちょっとちゃんと計算したとかいった計算したら、ちょっと違ってたら教えてほしいんすけど大体3.5倍ぐらいあったとアテンションのヘッドはこのFFNの次元数と同様にスケールしたモデルの次元と同様にスケールしているということがわかる。 こういった感じで幅とかを大体同じような係数で、なるべく伸ばしてくと、ただこれ、指定したパラメータ数にしようと思ったときに、当然どっかは完全には固定できないので、若干変わりますけど大体同じような比率でスケールさせているというようなことがわかると思います。\n"]}],"source":["# 本来は段落をそのままdocumentsに入れずに一定のサイズに分割した方が良いでしょうが、簡単のために段落をそのまま入れてしまいます。\n","documents = [text.replace(\"\\n\", \" \").strip() for text in raw_writedown.split(\"\\n\\n\")]\n","print(\"ドキュメントサイズ: \", len(documents))\n","print(\"ドキュメントの例: \\n\", documents[30])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FF6wc10RAxuc","outputId":"ff508c22-fe6a-4505-d10e-c56f9cc8183a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746520012527,"user_tz":-540,"elapsed":11742,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[64.81295013427734, 61.82796859741211, 62.283634185791016, 62.75822067260742, 63.076263427734375, 61.721134185791016, 59.56356430053711, 59.89995574951172, 61.3909797668457, 62.323463439941406, 61.017574310302734, 65.69007873535156, 65.84384155273438, 60.717262268066406, 62.97537612915039, 62.90790939331055, 61.83106994628906, 63.14445877075195, 60.53142547607422, 59.37460708618164, 59.9214973449707, 63.58799362182617, 59.79881286621094, 58.899688720703125, 62.603729248046875, 63.64401626586914, 61.17356872558594, 61.8225212097168, 60.26347732543945, 61.51750183105469, 64.97525787353516, 63.29969024658203, 63.340030670166016, 63.41764831542969, 65.70547485351562, 64.95523834228516, 58.454227447509766, 60.14152145385742, 60.078269958496094, 58.61556625366211, 57.923133850097656, 59.45297622680664, 58.90191650390625, 63.06171417236328, 68.84201049804688]]\n"]}],"source":["question = \"LLMにおけるInference Time Scalingとは？\"\n","\n","query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n","document_embeddings = emb_model.encode(documents)\n","\n","scores = (query_embeddings @ document_embeddings.T) * 100\n","print(scores.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H-FKkAcTA-Sx","outputId":"5c6d6bda-f481-4c65-e6f7-640552debbbf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746520012543,"user_tz":-540,"elapsed":3,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["取得したドキュメント1: (Score: 68.84201049804688)\n","最後に補足して僕のパート終わろうと思いますけど、同じ計算資源のときにパラメータ増やすのよりも推論資源を増やすのが有効なのかっていうのが問いとしてあると思いますけど、o1の場合だと、訓練時のスケールは同じままって推論時のスケールを増やしたら、より賢くなりましたって話でしたけど、どっちにするのがいいのかっていう意味で言うと、GoogleDeepMindが8月に論文としてまして、Scaling LLM Test-Time Comupte Optimally can be more Effective than Scaling More Paremetersっていうことで、良いらしいというふうに言われてます。厳密に言うとこれなんかタスクによって違うということなので、良いとまで言っていいのかちょっと若干誇大広告な気が個人的にはしてますけど、そういったことを検証するような研究も出てきていますので興味ある人は見てもらえばと思います。 \n","\n","\n","取得したドキュメント2: (Score: 65.84384155273438)\n","それから元のOpenAIの論文に戻りますと、今言ったようなLSTMの比較みたいなLSTMにおけるスケール則みたいなことも、この論文でも検証されていまして、左側がモデル構造が違うんですね。 Trasnformerの場合はスケール則が、パラメータ数が横軸になってますけどこういうふうになると、LSTMの場合には1層2層4層みたいにそれぞれスケール則を解くとこんなふうになりますよということで、Trasnformer以外のスケール則っていうのもあの検証をされている。深さについても検証してまして、これも元のモデルが何だったかちょっと忘れちゃったけど、確かLSTMだったような気がしますけど、層を変えたときにどういうふうな変化するかっていうのをこういった形でプロットするようなことがされてます。 ポイントはTrasnformer以外でも別にあのスケールするっていうのは成立概念だということです。なんでこんなTrasnformerだけ注目されてるのかってのは後で話します。 \n","\n","\n"]}],"source":["# 簡単のためにtop2でやります。結果を見てもらえれば問題なく関連する項目のみ取得できているのが分かるかと思います。\n","topk = 2\n","for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n","  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n","  print(documents[index], \"\\n\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtC-wsj4BGwn","outputId":"52f845c5-5093-4289-bce2-2edc0de695c3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746520023310,"user_tz":-540,"elapsed":10766,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]}],"source":["reference = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n","system_prompt = \"質問に回答してください。必ず「日本語で回答」すること。また、与えられる資料を参考にして回答すること。\"\n","question =  f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"\n","response = generate_output(question, system_prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c27VI95SCzV5","outputId":"7e6a59e5-5481-40b3-92ca-3684c8f1999b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746520023324,"user_tz":-540,"elapsed":12,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["LLM（Large Language Model）におけるInference Time Scalingは、推論時（Inference）に計算資源をスケールさせることで、性能を向上させることを指します。つまり、LLMが推論する際に、計算資源を増やすことで、より正確な結果を得ることができます。\n","\n","この概念は、Google DeepMindの論文「Scaling LLM Test-Time Compute Optimally can be more Effective than Scaling More Parameters」に基づいています。この論文では、LLMの推論時計算資源をスケールさせることで、性能を向上させることができ、パラメーターの増やすよりも有効なことが示されました。\n","\n","Inference Time Scalingは、LLMの推論時計算資源をスケールさせることで、性能を向上させることを目的としています。具体的には、LLMが推論する際に、計算資源を増やすことで、より正確な結果を得ることができます。\n"]}],"source":["print(response)"]},{"cell_type":"code","source":["# 評価 (openai apikeyがある場合のみ実行)\n","score = evaluate_answer_accuracy(question, response, gold_answer)\n","print(score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"so_dSyukCx6d","outputId":"c5cbafab-f458-4b50-fba4-c513318494df","executionInfo":{"status":"ok","timestamp":1746520024637,"user_tz":-540,"elapsed":1312,"user":{"displayName":"SAMMY KOJIKOJI","userId":"06710577592278940955"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.0\n"]}]},{"cell_type":"markdown","source":["＊＊＊講義動画だと3.0だった"],"metadata":{"id":"oY6El743tVnR"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"034d5e3b12fe47299d3551b2c6cc7ba9":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":[],"layout":"IPY_MODEL_2be7695d9d1e45a2b43537200b7ccbee"}},"0b89b22da8014d88b42e053f1948d733":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc8a630efde045f7bcd93f45afd3f8ac","placeholder":"​","style":"IPY_MODEL_aa282e5cb1094cd39a71e13eaabd23d6","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"74db5cad9fe34527baf7f471eeb44615":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_fa060b3dbffd4cbcb9cb1ed1fc9f7070","placeholder":"​","style":"IPY_MODEL_613982fde7664d47abcf79ff0f1763de","value":""}},"58acda839fac4aa0b4752394716c5a08":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_e805cf158a7341f0acda8a2f5d4a74cc","style":"IPY_MODEL_28aea689611842b6b3319c340567d0e9","value":true}},"54fabe8a9d814cd9921d8f8ec379c043":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_0b898699a1ee4b3c9c206785a72df150","style":"IPY_MODEL_967c322ce9b442b5a04db26aad5b28b2","tooltip":""}},"6d1c373435b54749a7fad388845318da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d3643f6da5a48da883a9381df94d02d","placeholder":"​","style":"IPY_MODEL_cc0c003783d742ff8a503193a20af719","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"2be7695d9d1e45a2b43537200b7ccbee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"dc8a630efde045f7bcd93f45afd3f8ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa282e5cb1094cd39a71e13eaabd23d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa060b3dbffd4cbcb9cb1ed1fc9f7070":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"613982fde7664d47abcf79ff0f1763de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e805cf158a7341f0acda8a2f5d4a74cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28aea689611842b6b3319c340567d0e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b898699a1ee4b3c9c206785a72df150":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"967c322ce9b442b5a04db26aad5b28b2":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"5d3643f6da5a48da883a9381df94d02d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc0c003783d742ff8a503193a20af719":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e68b5f2589424d268a38aea24e963320":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd1aea1142f54efa9f17c60311585c03","placeholder":"​","style":"IPY_MODEL_93ea6b73f32c4e8ba62ae6e63a0806ce","value":"Connecting..."}},"bd1aea1142f54efa9f17c60311585c03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93ea6b73f32c4e8ba62ae6e63a0806ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90392e6635944c9e85c0c0b47a7fc9f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_97733a01c952496d9e9301f3ec1a8462","IPY_MODEL_779bdedbfcf8478b96401b142b95e066","IPY_MODEL_54aa3ad26c024a1699f05e06134e1d7a"],"layout":"IPY_MODEL_910e6f4076684b97bd353520f6f09b33"}},"97733a01c952496d9e9301f3ec1a8462":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0c43d43dadb4d83952dada98668bbe5","placeholder":"​","style":"IPY_MODEL_0409b85c4c6a4be9b1ac090ca13906e0","value":"Loading checkpoint shards: 100%"}},"779bdedbfcf8478b96401b142b95e066":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e3302aa33fc4716ab2139264a897029","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_295ad0f4acff4ac3acc249bbf0b34fe4","value":4}},"54aa3ad26c024a1699f05e06134e1d7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19262dc7c9c64f0d801a3e1d0829d007","placeholder":"​","style":"IPY_MODEL_33318c18abc146fdb5daa5fd3a47849d","value":" 4/4 [00:16&lt;00:00,  3.51s/it]"}},"910e6f4076684b97bd353520f6f09b33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0c43d43dadb4d83952dada98668bbe5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0409b85c4c6a4be9b1ac090ca13906e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e3302aa33fc4716ab2139264a897029":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"295ad0f4acff4ac3acc249bbf0b34fe4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"19262dc7c9c64f0d801a3e1d0829d007":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33318c18abc146fdb5daa5fd3a47849d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}